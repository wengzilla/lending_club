{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import pdb \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import util\n",
    "import seaborn as sns \n",
    "import clean_data as cd\n",
    "import benchmark\n",
    "import group_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/2.7/site-packages/IPython/core/interactiveshell.py:3006: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if self.run_code(code, result):\n"
     ]
    }
   ],
   "source": [
    "pd2007 = cd.import_data(2007)\n",
    "pd2012 = cd.import_data(2012)\n",
    "\n",
    "all_data = pd.concat([pd2007, pd2012])\n",
    "prepped_data = cd.prep_for_classification(all_data)\n",
    "X, y = cd.feature_target_split(prepped_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmark for all data on A/B/C: 1.08\n",
      "Benchmar for all data on all loans: 1.08\n",
      "Benchmark for all data is : 1.08\n",
      "Benchmark for 2012 data is: 1.09\n",
      "Benchmark for 2007 data is: 1.08\n",
      "Making group for group A\n",
      "Making group for group B\n",
      "Making group for group C\n",
      "Making group for group D\n",
      "Making group for group E\n",
      "Making group for group F\n",
      "Making group for group G\n",
      "Precision for group A: 0.99\n",
      "Benchmark for group A: 1.09\n",
      "[[5999   36]\n",
      " [  36  345]]\n",
      "Precision for group B: 0.99\n",
      "Benchmark for group B: 1.16\n",
      "[[9318  362]\n",
      " [ 121 1244]]\n",
      "Precision for group C: 0.77\n",
      "Benchmark for group C: 0.99\n",
      "[[  17 5594]\n",
      " [   5 1212]]\n",
      "Precision for group D: 1.00\n",
      "Benchmark for group D: 1.15\n",
      "[[   1 2855]\n",
      " [   0  848]]\n",
      "Precision for group E: 0.60\n",
      "Benchmark for group E: 0.58\n",
      "[[  3 582]\n",
      " [  2 187]]\n",
      "Precision for group F: nan\n",
      "Benchmark for group F: nan\n",
      "[[ 0 79]\n",
      " [ 0 41]]\n",
      "Precision for group G: nan\n",
      "Benchmark for group G: nan\n",
      "[[ 0 11]\n",
      " [ 0  4]]\n",
      "Overall precision: 15338 164\n",
      "Overall benchmark: 1.13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/2.7/site-packages/numpy/core/fromnumeric.py:2652: VisibleDeprecationWarning: `rank` is deprecated; use the `ndim` attribute or function instead. To find the rank of a matrix see `numpy.linalg.matrix_rank`.\n",
      "  VisibleDeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# grades are still here\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "train_payouts = all_data.ix[X_train.index]['payout_prop']\n",
    "test_payouts = all_data.ix[X_test.index]['payout_prop']\n",
    "\n",
    "print \"Benchmark for all data on A/B/C: %.2f\" %benchmark.total_return(all_data, filter_col='grade', filter_vals=['A', 'B', 'C'])\n",
    "print \"Benchmar for all data on all loans: %.2f\" %benchmark.total_return(all_data)\n",
    "print \"Benchmark for all data is : %.2f\" %np.mean(all_data['payout_prop'])\n",
    "print \"Benchmark for 2012 data is: %.2f\" %np.mean(pd2012['payout_prop'])\n",
    "print \"Benchmark for 2007 data is: %.2f\" %np.mean(pd2007['payout_prop'])\n",
    "\n",
    "lr_params = { \n",
    "    'C': 100,\n",
    "    'class_weight': {False: 15},\n",
    "    'penalty': 'l2',\n",
    "    'n_jobs': -1\n",
    "}\n",
    "\n",
    "models = group_models.fit(X_train, y_train, 'grade', LogisticRegression, lr_params)\n",
    "group_models.group_results(X_test, y_test, test_payouts, 'grade', models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=100, class_weight={False: 15}, dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='ovr', n_jobs=-1, penalty='l2', random_state=None,\n",
       "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_lr_model = LogisticRegression(**lr_params)\n",
    "all_lr_model.fit(X_train.drop('grade', axis=1), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predicted = all_lr_model.predict(X_test.drop('grade', axis=1))\n",
    "cm_all_model = confusion_matrix(y_test, predicted, labels=[True, False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.988283339005\n"
     ]
    }
   ],
   "source": [
    "precision_all = cm_all_model[0,0] / np.sum(cm_all_model[:, 0])\n",
    "print precision_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1582344506470594"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_model_indices = [X_test.index[i] for i in range(len(X_test)) if predicted[i] == 1]\n",
    "predicted_df = all_data.ix[all_model_indices]\n",
    "benchmark.total_return(predicted_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[20328  4529]\n",
      " [  241  3804]]\n"
     ]
    }
   ],
   "source": [
    "print cm_all_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(' Total pred 1: ', 6063)\n",
      "Group A\n",
      "[[6001   34]\n",
      " [  62  319]]\n",
      "Precision: 0.99\n",
      "=======================================================\n",
      "(' Total pred 1: ', 9055)\n",
      "Group B\n",
      "[[8938  742]\n",
      " [ 117 1248]]\n",
      "Precision: 0.99\n",
      "=======================================================\n",
      "(' Total pred 1: ', 3986)\n",
      "Group C\n",
      "[[3936 1675]\n",
      " [  50 1167]]\n",
      "Precision: 0.99\n",
      "=======================================================\n",
      "(' Total pred 1: ', 1298)\n",
      "Group D\n",
      "[[1287 1569]\n",
      " [  11  837]]\n",
      "Precision: 0.99\n",
      "=======================================================\n",
      "(' Total pred 1: ', 150)\n",
      "Group E\n",
      "[[149 436]\n",
      " [  1 188]]\n",
      "Precision: 0.99\n",
      "=======================================================\n",
      "(' Total pred 1: ', 16)\n",
      "Group F\n",
      "[[16 63]\n",
      " [ 0 41]]\n",
      "Precision: 1.00\n",
      "=======================================================\n",
      "(' Total pred 1: ', 1)\n",
      "Group G\n",
      "[[ 1 10]\n",
      " [ 0  4]]\n",
      "Precision: 1.00\n",
      "=======================================================\n"
     ]
    }
   ],
   "source": [
    "test_copy = X_test.copy()\n",
    "test_copy['prediction'] = predicted\n",
    "grouped_test = test_copy.groupby('grade')\n",
    "\n",
    "for gname, df in grouped_test:\n",
    "    y_group = y_test.ix[df.index]\n",
    "    print (\" Total pred 1: \", np.sum(df['prediction']))\n",
    "    cm_group = confusion_matrix(y_group, df['prediction'], labels=[True, False])\n",
    "    print \"Group %s\" %gname\n",
    "    print cm_group\n",
    "    print \"Precision: %.2f\" %(float(cm_group[0, 0]) / float(cm_group[0,0] + cm_group[1, 0]))\n",
    "    print(\"=======================================================\")\n",
    "    # grab rows by grade\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
